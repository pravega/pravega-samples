
Event pattern detection with Apache Flink and Pravega
=====================================================

This is a sample application which simulates network anomaly intrusion and how it can be detected with Apache Flink and Pravega.

Events in streams (generated by devices and services, such as firewalls login-, and authentication services) are expected to occur in certain patterns. Any deviation from these patterns indicates an anomaly (attempted intrusion) that the streaming system should recognize and that should trigger an alert.

The event patterns are tracked per interacting party (here simplified per source IP address) and are validated by a state machine. The state machine's states define what possible events may occur next, and what new states these events will result in.
 
The final aggregated results are grouped under network id which acts as a network domain abstraction hosting multiple server machines.

The following diagram depicts the state machine used in this example.

```
           +--<a>--> W --<b>--> Y --<e>---+
           |                    ^         |     +-----<g>---> TERM
   INITIAL-+                    |         |     |
           |                    |         +--> (Z)---<f>-+
           +--<c>--> X --<b>----+         |     ^        |
                     |                    |     |        |
                     +--------<d>---------+     +--------+
```

Getting Started
---------------


### Building Pravega

_Be sure to use the `r0.0-alpha` branch for compatibility with the Alpha release._

Install the Pravega client libraries to your local Maven repository:
```
$./gradlew publishMavenPublicationToMavenLocal connectors:flink:publishShadowPublicationToMavenLocal
```

### Building and Running Example

Clone the repository and run `mvn clean build` to build the repository.

<b>To run the application from command line using gradle</b>

`./gradlew anomaly-detection:run -PargsList="--configDir <APP_CONFIG_DIR> --mode <RUN_MODE>"` where
 
`APP_CONFIG_DIR` is the directory that contains the `app.json` configuration file
 
`RUN_MODE` can take any of the 3 options (1, 2 or 3), 

1 = Publish events to Pravega, 

2 = Run Anomaly detection, 

3 = Publish and Run Anomaly detection from in-memory source (for dev purpose)

4 = Create pravega stream which is provided in the configuration file
  
<b>To run the application from Nautilus Lab environment</b>

1) Run `./gradlew upload` which will upload the artifacts to master node
2) SSH to the master node `centos@<IP>` 
3) Under the home directory, you will see a folder `pravega-flink-anomaly-detection`

After changing the configuration file entries (see below), you can run below commands to publish events to Pravega and consume perform anomaly detection from Pravega streams

`bin/anomaly-detection --configDir /home/centos/pravega-flink-anomaly-detection/conf --mode 1`

`bin/anomaly-detection --configDir /home/centos/pravega-flink-anomaly-detection/conf --mode 2`

<b>To deploy and run the application from Nautilus cluster</b>

1) Run `./gradlew upload` which will upload the artifacts to master node
2) SSH to the master node `centos@<IP>` 
3) Under the home directory, you will see a folder `pravega-flink-anomaly-detection`

After changing the configuration file entries (see below), you can run below commands to publish events to Pravega and consume perform anomaly detection from Pravega streams

`bin/run --configDir /home/centos/pravega-flink-anomaly-detection/conf --mode 1`

`bin/run --configDir /home/centos/pravega-flink-anomaly-detection/conf --mode 2`

For better results, run both producer and consumer application on two separate window to see the error record simulation and detection happening in real time. 

Application Configuration
-------------------------

```
{
  "name": "anomaly-detection",
  "producer": {
    "latencyInMilliSec": 100,
    "capacity": 1000,
    "errorProbFactor": 0.007
  },
  "pipeline": {
    "parallelism": 1,
    "checkpointIntervalInMilliSec": 1000,
    "disableCheckpoint": false,
    "watermarkOffsetInSec": 5,
    "windowIntervalInSeconds": 30,
    "elasticSearch": {
      "sinkResults": true,
      "host": "127.0.0.1",
      "port": 9300,
      "cluster": "elasticsearch",
      "index": "anomaly-index",
      "type":"anomalies"
    }
  },
  "pravega": {
    "controllerUri": "tcp://127.0.0.1:9090",
    "stream": "NetworkPacket",
    "scope": "Network",
    "writer": {
      "routingKey": "NetworkEvent"
    }
  }
}

```

`latencyInMilliSec` - how frequently events needs to be generated and published to Pravega

`capacity` - initial capacity till which the error records will not be generated

`errorProbFactor` - how frequently error records needs to be simulated. Provide a value between 0.1 to 1

`parallelism` - Flink job parallelism

`controllerUri` - change the value to `tcp://controller-pravega.marathon.mesos:9091` while testing in Nautilus cluster
 
 `elasticSearch` - Final results will be sinked to elasticsearch if `sinkResults` is set to `true`
 
 `windowIntervalInSeconds` - Window frequency interval
 
 `watermarkOffsetInSec` - Window watermark offset interval
 
 
 Standalone Deployment
 ---------------------
 
 Follow below steps to deploy and run both Pravega and the example (including elasticsearch/kibana). 
 
 The steps are tested on Ubuntu 16.x OS/docker 1.13.1/JDK8
 
**Run Pravega**

Run `./gradlew startSinglenode` to start `Pravega` on stand-alone mode from the root of Pravega cloned repository
 
**Install Elastic Search and Kibana**

```
Install Elastic Search
sudo mkdir -p /elk-stack/esdata
sudo chmod 777 -R /elk-stack/
docker run -d --name elasticsearch  -p 9200:9200 -p 9300:9300 -v /elk-stack/esdata:/usr/share/elasticsearch/data elasticsearch:2.4

Verify if ES is running by executing the command: 
curl -X GET http://localhost:9200

Install Kibana
docker run --name kibana --link elasticsearch:elasticsearch -p 5601:5601 -d kibana

Verfiy by going to the URL: http://localhost:5601

```

**Create Elastic Search Index and define schema**
```

curl -XDELETE "http://localhost:9200/anomaly-index"

curl -XPUT "http://localhost:9200/anomaly-index"

curl -XPUT "http://localhost:9200/anomaly-index/_mapping/anomalies" -d'
{
 "anomalies" : {
   "properties" : {
     "count": {"type": "integer"},
     "location": {"type": "geo_point"},
     "minTimestampAsString": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss z"},
     "maxTimestampAsString": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss z"}
   }
 }
}'

```

**Run Sample Application**

```
Running below command will create Pravega stream as provided in the app.json 
./gradlew anomaly-detection:run -PargsList="--configDir /softlinks --mode 4"

Running below command will start pumping sample data to Pravega
./gradlew anomaly-detection:run -PargsList="--configDir /softlinks --mode 1"

Running below command will start the anomaly detection application
./gradlew anomaly-detection:run -PargsList="--configDir /softlinks --mode 2"

```

**Visualize results in Kibana**

```
- Create index pattern for "anomaly-index" and set it as default index
- Visualize the metrics using "Tile Map" option and choose "sum" aggregation on the field "count"
- Select Geo-Coordinates bucket for the field  "location"
- You can now visualize the total anomalies per geo location for the simulated time window period

```

The following screenshot shows how Kibana visualizes the result:
![Kibana Screenshot](./src/main/resources/Network-Anomaly.png?raw=true "Kibana Screenshot")
