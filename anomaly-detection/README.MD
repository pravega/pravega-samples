
# Event pattern detection with Apache Flink and Pravega


Sample application which simulates network anomaly intrusion and detection using Apache Flink and Apache Pravega.

This application is based on [streaming-state-machine](https://github.com/StephanEwen/flink-demos/tree/master/streaming-state-machine) which is slightly extended to demonstrate Pravega/Flink integration capabilities.

Events in streams (generated by devices and services, such as firewalls, routers, authentication services etc.,) are expected to occur in certain patterns. Any deviation from these patterns indicates an anomaly (attempted intrusion) that the streaming system should recognize and that should trigger an alert.

The event patterns are tracked per interacting party (here simplified per source IP address) and are validated by a state machine. The state machine's states define what possible events may occur next, and what new states these events will result in.
 
The final aggregated results are grouped under network id which acts as a network domain abstraction hosting multiple server machines.

The aggregated results are sinked to Elastic Search for visualizing from Kibana user interface.

The following diagram depicts the state machine used in this example.

```
           +--<a>--> W --<b>--> Y --<e>---+
           |                    ^         |     +-----<g>---> TERM
   INITIAL-+                    |         |     |
           |                    |         +--> (Z)---<f>-+
           +--<c>--> X --<b>----+         |     ^        |
                     |                    |     |        |
                     +--------<d>---------+     +--------+
```

## Getting Started


### Building Pravega & Flink connectors

Follow below steps to build and publish artifacts from source to local Maven repository:

```
$git clone https://github.com/pravega/pravega.git
$./gradlew clean install

$git clone https://github.com/pravega/flink-connectors.git
$./gradlew clean install

```
Alternatively, follow the instructions from [here](http://pravega.io/docs/getting-started/) to pull from release repository.

### Building and Running Example

- Clone the repository (git clone https://github.com/pravega/pravega-samples.git)
- Build samples `./gradlew clean build`
- Create local distribution `./gradlew clean installDist`

<b>To run the application using script (Embedded Flink App)</b>

Navigate to `$PRAVEGA_SAMPLES_HOME/anomaly-detection/build/install/pravega-flink-anomaly-detection/`

Update `conf/app.json` (see "Application Configuration" section) as appropriate and run below commands to create stream and publish/consume records.

```
bin/anomaly-detection --configDir <APP_CONFIG_DIR> --mode <RUN_MODE>

where, 

- `APP_CONFIG_DIR` is the directory that contains the `app.json` configuration file
- `RUN_MODE` can take any of the 3 options (1, 2 or 3),
  - 1 = Create pravega stream and Scope using info from configuration file
  - 2 = Publish events to Pravega, 
  - 3 = Run Anomaly detection, 
```

<b>To run the application using script (On top of existing Flink cluster)</b>

Navigate to `$PRAVEGA_SAMPLES_HOME/anomaly-detection/build/install/pravega-flink-anomaly-detection/`

Update `conf/app.json` (see "Application Configuration" section) as appropriate and run below commands to create stream and publish/consume records.

```
1) Create Pravega stream
`bin/anomaly-detection --configDir <APP_CONFIG_DIR> --mode 1`

2) Publish events to Pravega

`flink run -c io.pravega.anomalydetection.ApplicationMain lib/pravega-flink-anomaly-detection-0.1.0-SNAPSHOT-all.jar --configDir conf --mode 2` 
 (or) 
`bin/anomaly-detection --configDir <APP_CONFIG_DIR> --mode 2`

(The former will run the producer as Flink app and the later will run as a console application. Use console app option if you want to simulate error record manually by setting **controlledEnv** flag to true)

3) Run stream job
`flink run -c io.pravega.anomalydetection.ApplicationMain lib/pravega-flink-anomaly-detection-0.1.0-SNAPSHOT-all.jar --configDir conf --mode 3`

Ensure $FLINK_HOME/bin is in the PATH to use bin/run command option.

```

## Testing State Recovery
Perform below steps to test the job state recovery. This works only when checkpoint is enabled and a reliable state backend is configured to store checkpoint/savepoint snapshot (flink-conf.yaml).

1) Cancel the current running job using `-s` option which will create a savepoint in the default configured savepoint location as defined in flink-config.yaml

```
bin/flink cancel -s <JOB_ID>
```

With the above command, the job will be cancelled and the application's current state will be saved in backed filesystem.

2) To resume from a savepoint run the below command

```
bin/flink run -s <SAVEPOINT_LOCATION> -c io.pravega.anomalydetection.ApplicationMain <PATH_TO_pravega-flink-anomaly-detection-0.1.0-SNAPSHOT-all.jar> --configDir <APP_CONFIG_DIR> --mode 3
```
The job should nicely recover from the last checkpointed state. In otherwords, you should not see any alerts (duplicate) that are already processed before taking the snapshot. The alerts can be either visualized from Kibana or from Taskmanager log file.  


## Application Configuration

```
{
  "name": "anomaly-detection",
  "producer": {
    "latencyInMilliSec": 2000,
    "capacity": 100,
    "errorProbFactor": 0.3,
    "controlledEnv": false
  },
  "pipeline": {
    "parallelism": 1,
    "checkpointIntervalInMilliSec": 1000,
    "disableCheckpoint": false,
    "watermarkOffsetInSec": 0,
    "windowIntervalInSeconds": 30,
    "elasticSearch": {
      "sinkResults": false,
      "host": "127.0.0.1",
      "port": 9300,
      "cluster": "elasticsearch",
      "index": "anomaly-index",
      "type":"anomalies"
    }
  },
  "pravega": {
    "controllerUri": "tcp://127.0.0.1:9090",
    "stream": "NetworkPacket",
    "scope": "Network"
  }
}

```

`latencyInMilliSec` - how frequently events needs to be generated and published to Pravega

`capacity` - initial capacity till which the error records will not be generated

`errorProbFactor` - how frequently error records needs to be simulated. Provide a value between 0.1 to 1

`controlledEnv` - When this is true, the `errorProbFactor` value will be ignored and the error record will be generated only when user sends request (Possible Options: `S` to simulate error record, `Q` to quit ingestion)

`parallelism` - This value will be used to define pravega segment count/fixed scaling policy and Flink job parallelism

`controllerUri` - Pravega controller endpoint
 
`elasticSearch` - Final results will be sinked to elasticsearch if `sinkResults` is set to `true`
 
`windowIntervalInSeconds` - Window frequency interval
 
`watermarkOffsetInSec` - Window watermark offset interval

`disableCheckpoint` - If checkpoint is enabled, make sure Flink cluster uses appropriate state backend (checkpoint/savepoint) configurations
 
 
 ### Steps to setup Elastic Search/Kibana
 
 
The steps are tested on Ubuntu 16.x OS/docker 1.13.1/JDK8/ElasticSearch5/Kibana5.3.0
 
**Install Elastic Search and Kibana**

```
Install Elastic Search
sudo mkdir -p /elk-stack/esdata
sudo chmod 777 -R /elk-stack/
docker run -d --name elasticsearch  --net="host" -v /elk-stack/esdata:/usr/share/elasticsearch/data elasticsearch

Verify if ES is running by executing the command: 
curl -X GET http://localhost:9200

update /etc/hosts with 127.0.0.1 elasticsearch (this is for Kibana to lookup ES)

Install Kibana
docker run -d --name kibana --net="host" kibana

Verify by going to the URL: http://localhost:5601
```

**Create Elastic Search Index and define schema**

```
curl -XDELETE "http://localhost:9200/anomaly-index"

curl -XPUT "http://localhost:9200/anomaly-index"

curl -XPUT "http://localhost:9200/anomaly-index/_mapping/anomalies" -d'
{
 "anomalies" : {
   "properties" : {
     "count": {"type": "integer"},
     "location": {"type": "geo_point"},
     "minTimestampAsString": {"type": "date"},
     "maxTimestampAsString": {"type": "date"}
   }
 }
}'
```

**Visualize results in Kibana**

```
- Create index pattern for "anomaly-index" and set it as default index
- Visualize the metrics using "Tile Map" option and choose "sum" aggregation on the field "count"
- Select Geo-Coordinates bucket for the field  "location"
- You can now visualize the total anomalies per geo location for the simulated time window period
```

Final results filtered based on lat/long coordinates from Kibana
![Kibana Screenshot](./src/main/resources/Network-Anomaly.png?raw=true "Kibana Screenshot")