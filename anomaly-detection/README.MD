
# Event pattern detection with Apache Flink and Pravega


Sample application which simulates network anomaly intrusion and detection using Apache Flink and Apache Pravega.

This application is based on [streaming-state-machine](https://github.com/StephanEwen/flink-demos/tree/master/streaming-state-machine) which is slightly extended to demonstrate Pravega/Flink integration capabilities.

Events in streams (generated by devices and services, such as firewalls login-, and authentication services) are expected to occur in certain patterns. Any deviation from these patterns indicates an anomaly (attempted intrusion) that the streaming system should recognize and that should trigger an alert.

The event patterns are tracked per interacting party (here simplified per source IP address) and are validated by a state machine. The state machine's states define what possible events may occur next, and what new states these events will result in.
 
The final aggregated results are grouped under network id which acts as a network domain abstraction hosting multiple server machines.

The aggregated results are sinked to Elastic Search for visualizing from Kibana user interface.

The following diagram depicts the state machine used in this example.

```
           +--<a>--> W --<b>--> Y --<e>---+
           |                    ^         |     +-----<g>---> TERM
   INITIAL-+                    |         |     |
           |                    |         +--> (Z)---<f>-+
           +--<c>--> X --<b>----+         |     ^        |
                     |                    |     |        |
                     +--------<d>---------+     +--------+
```

## Getting Started


### Building Pravega & Flink connectors

Follow below steps to build and publish artifacts from source to local Maven repository:

```
$git clone https://github.com/pravega/pravega.git
$./gradlew clean build
$./gradlew install

$git clone https://github.com/pravega/flink-connectors.git
$./gradlew clean build
$./gradlew install

```
Alternatively, follow the instructions from [here](http://pravega.io/docs/getting-started/) to pull from release repository.

### Building and Running Example

- Clone the repository 
- Build samples `mvn clean build`
- Create local distribution `mvn clean installDist`

<b>To run the application using script (Embedded Flink App)</b>

Navigate to `$PRAVEGA_SAMPLES_HOME/anomaly-detection/build/install/pravega-flink-anomaly-detection/`

Update `conf/app.json` (see "Application Configuration" section) as appropriate and run below commands to create stream and publish/consume records.

```
`bin/anomaly-detection --configDir <APP_CONFIG_DIR> --mode <RUN_MODE>`

where, 

- `APP_CONFIG_DIR` is the directory that contains the `app.json` configuration file
- `RUN_MODE` can take any of the 3 options (1, 2 or 3),
  - 1 = Create pravega stream and Scope using info from configuration file
  - 2 = Publish events to Pravega, 
  - 3 = Run Anomaly detection, 
```

Alternatively, you could also use `gradle run` to run from `$PRAVEGA_SAMPLES_HOME` directory

```
`./gradlew anomaly-detection:run -PargsList="--configDir <APP_CONFIG_DIR> --mode <RUN_MODE>"`
``` 

<b>To run the application using script (On top of existing Flink cluster)</b>

Navigate to `$PRAVEGA_SAMPLES_HOME/anomaly-detection/build/install/pravega-flink-anomaly-detection/`

Update `conf/app.json` (see "Application Configuration" section) as appropriate and run below commands to create stream and publish/consume records.

```
`bin/anomaly-detection --configDir <APP_CONFIG_DIR>--mode 1`

`bin/run --configDir <APP_CONFIG_DIR> --mode 2` or `bin/anomaly-detection --configDir <APP_CONFIG_DIR>--mode 2`
(The former will run the producer as Flink app and the later will run as consoler app. Use console app option if you want to simulate error record manually by setting **controlledEnv** flag to true)

`bin/run --configDir /home/centos/pravega-flink-anomaly-detection/conf --mode 3`

Ensure $FLINK_HOME/bin is in the PATH to use bin/run command option.

```

## Application Configuration


```
{
  "name": "anomaly-detection",
  "producer": {
    "latencyInMilliSec": 2000,
    "capacity": 100,
    "errorProbFactor": 0.3,
    "controlledEnv": false
  },
  "pipeline": {
    "parallelism": 1,
    "checkpointIntervalInMilliSec": 1000,
    "stateCheckpointDir": "hdfs://127.0.0.1:8020/checkpoint",
    "disableCheckpoint": true,
    "watermarkOffsetInSec": 0,
    "windowIntervalInSeconds": 30,
    "elasticSearch": {
      "sinkResults": true,
      "host": "127.0.0.1",
      "port": 9300,
      "cluster": "elasticsearch",
      "index": "anomaly-index",
      "type":"anomalies"
    }
  },
  "pravega": {
    "controllerUri": "tcp://127.0.0.1:9090",
    "stream": "NetworkPacket",
    "scope": "Network",
    "writer": {
      "routingKey": "NetworkEvent"
    }
  }
}

```

`latencyInMilliSec` - how frequently events needs to be generated and published to Pravega

`capacity` - initial capacity till which the error records will not be generated

`errorProbFactor` - how frequently error records needs to be simulated. Provide a value between 0.1 to 1

`controlledEnv` - When this is true, the `errorProbFactor` value will be ignored and the error record will be generated only when user sends request (Possible Options: `S` to simulate error record, `Q` to quit ingestion)

`parallelism` - Flink job parallelism

`controllerUri` - Pravega controller endpoint
 
`elasticSearch` - Final results will be sinked to elasticsearch if `sinkResults` is set to `true`
 
`windowIntervalInSeconds` - Window frequency interval
 
`watermarkOffsetInSec` - Window watermark offset interval
 
`stateCheckpointDir` - HDFS FS URI along with the checkpoint directory name
 
 
 ### Steps to setup Elastic Search/Kibana
 
 
The steps are tested on Ubuntu 16.x OS/docker 1.13.1/JDK8/ElasticSearch5/Kibana5.3.0
 
**Install Elastic Search and Kibana**

```
Install Elastic Search
sudo mkdir -p /elk-stack/esdata
sudo chmod 777 -R /elk-stack/
docker run -d --name elasticsearch  --net="host" -v /elk-stack/esdata:/usr/share/elasticsearch/data elasticsearch

Verify if ES is running by executing the command: 
curl -X GET http://localhost:9200

update /etc/hosts with 127.0.0.1 elasticsearch (this is for Kibana to lookup ES)

Install Kibana
docker run -d --name kibana --net="host" kibana

Verfiy by going to the URL: http://localhost:5601
```

**Create Elastic Search Index and define schema**

```
curl -XDELETE "http://localhost:9200/anomaly-index"

curl -XPUT "http://localhost:9200/anomaly-index"

curl -XPUT "http://localhost:9200/anomaly-index/_mapping/anomalies" -d'
{
 "anomalies" : {
   "properties" : {
     "count": {"type": "integer"},
     "location": {"type": "geo_point"},
     "minTimestampAsString": {"type": "date"},
     "maxTimestampAsString": {"type": "date"}
   }
 }
}'
```

**Visualize results in Kibana**

```
- Create index pattern for "anomaly-index" and set it as default index
- Visualize the metrics using "Tile Map" option and choose "sum" aggregation on the field "count"
- Select Geo-Coordinates bucket for the field  "location"
- You can now visualize the total anomalies per geo location for the simulated time window period
```

Final results filtered based on lat/long coordinates from Kibana
![Kibana Screenshot](./src/main/resources/Network-Anomaly.png?raw=true "Kibana Screenshot")